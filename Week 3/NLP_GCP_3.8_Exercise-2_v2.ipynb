{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Class 3 Exercise 1:\n",
    "- You are provided with a following sentence: *\"I have a `fazt` car\"*, where `fazt` is the misspelled word\n",
    "- Create a bi-gram model, that looks at the prior word to misspelled word and provides suggested correction\n",
    "- Create a tri-gram model, that looks at the prior word as well as following word to misspelled word and provides suggested correction\n",
    "- Improve the performance of your spelling recommendations by only considering the tokens within edit distance of 1 for misspelled word  \n",
    "**Suggestions:** \n",
    "- You can use either NLTL Brown Corpus or NLTK Reuters Corpus to build a model\n",
    "\n",
    "`Brown Corpus`: a collection of texts from a wide range of sources, all written in 1961 created at Brown University.  It includes texts from 500 sources and covers ~1.1M words, where the sources have been categorized by genre, for example, news, editorial, adventure fiction, mystery fiction, romance, etc.  \n",
    "`Reuters Corpus`: contains ~10K news documents totaling over 1.7 million words.  The documents in this corpus are from the Reuters newswire in the late 1980s. They have been classified into 90 topics, and thus, the corpus is often used for experiments in text categorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-01-19T02:36:39.714039Z",
     "start_time": "2024-01-19T02:36:38.443671Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/ziyuanye/nltk_data...\n",
      "[nltk_data] Downloading package brown to /Users/ziyuanye/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %pip install nltk==3.8.1\n",
    "\n",
    "import nltk\n",
    "nltk.download('reuters', halt_on_error=False)\n",
    "nltk.download('brown', halt_on_error=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling correction using Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T02:36:15.816273Z",
     "start_time": "2024-01-19T02:36:11.914833Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import ngrams, bigrams, trigrams\n",
    "from nltk.corpus import reuters, brown\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Download the necessary datasets\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create most likely corrections, purely based on ngram frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-01-19T02:37:22.887195Z",
     "start_time": "2024-01-19T02:37:11.733321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size brown_words: 1,161,192\n",
      "Corpus size reuters_words: 1,720,901\n"
     ]
    }
   ],
   "source": [
    "# Get tokens from NLTK Corpus\n",
    "brown_words = brown.words() # for Brown corpus\n",
    "reuters_words = reuters.words() # for Reuters corpus\n",
    "print(f'Corpus size brown_words: {len(brown_words):,}')\n",
    "print(f'Corpus size reuters_words: {len(reuters_words):,}')\n",
    "\n",
    "# Choose your preferred corpus\n",
    "words = [word.lower() for word in brown_words] \n",
    "# words = [word.lower() for word in reuters_words]\n",
    "\n",
    "# Eliminate punctuation from corpus\n",
    "filtered_words = [word for word in words if not re.fullmatch(r'[^\\w\\s]', word)]\n",
    "\n",
    "# Create bigrams and trigrams from the corpus\n",
    "corpus_bigrams = list(bigrams(filtered_words))\n",
    "corpus_trigrams = list(trigrams(filtered_words))\n",
    "\n",
    "# Get frequencies of bigrams and trigrams in the corpus\n",
    "bigram_freq = Counter(corpus_bigrams)\n",
    "trigram_freq = Counter(corpus_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T02:37:34.935872Z",
     "start_time": "2024-01-19T02:37:33.797613Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         Bigram  Frequency\n40    (of, the)       9721\n83    (in, the)       6041\n158   (to, the)       3492\n401   (on, the)       2477\n117  (and, the)       2247",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Bigram</th>\n      <th>Frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>40</th>\n      <td>(of, the)</td>\n      <td>9721</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>(in, the)</td>\n      <td>6041</td>\n    </tr>\n    <tr>\n      <th>158</th>\n      <td>(to, the)</td>\n      <td>3492</td>\n    </tr>\n    <tr>\n      <th>401</th>\n      <td>(on, the)</td>\n      <td>2477</td>\n    </tr>\n    <tr>\n      <th>117</th>\n      <td>(and, the)</td>\n      <td>2247</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                    Trigram  Frequency\n334          (one, of, the)        404\n6294  (the, united, states)        336\n6657         (as, well, as)        238\n2263         ('', he, said)        222\n1183        (some, of, the)        179",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Trigram</th>\n      <th>Frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>334</th>\n      <td>(one, of, the)</td>\n      <td>404</td>\n    </tr>\n    <tr>\n      <th>6294</th>\n      <td>(the, united, states)</td>\n      <td>336</td>\n    </tr>\n    <tr>\n      <th>6657</th>\n      <td>(as, well, as)</td>\n      <td>238</td>\n    </tr>\n    <tr>\n      <th>2263</th>\n      <td>('', he, said)</td>\n      <td>222</td>\n    </tr>\n    <tr>\n      <th>1183</th>\n      <td>(some, of, the)</td>\n      <td>179</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a DataFrame for the bigrams\n",
    "df_bigram = pd.DataFrame(list(bigram_freq.items()), columns=['Bigram', 'Frequency'])\n",
    "df_bigram.sort_values(by='Frequency', ascending=False, inplace=True)\n",
    "display(df_bigram.head(5))\n",
    "\n",
    "# Create a DataFrame for the trigrams\n",
    "df_trigram = pd.DataFrame(list(trigram_freq.items()), columns=['Trigram', 'Frequency'])\n",
    "df_trigram.sort_values(by='Frequency', ascending=False, inplace=True)\n",
    "display(df_trigram.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mon, 26 June 2023 11:43:31'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "\n",
    "datetime.datetime.now(pytz.timezone('US/Central')).strftime(\"%a, %d %B %Y %H:%M:%S\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m104"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
