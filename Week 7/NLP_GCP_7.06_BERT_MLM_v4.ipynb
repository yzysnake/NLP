{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/transformers/pretrained_models.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, logging\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "logging.get_verbosity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model inference works fine on both CPUs and GPUs.  For fine-tuning GPUs are necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    use_device = 'GPU'\n",
    "else:\n",
    "    use_device = 'CPU'\n",
    "    \n",
    "# use_device = 'CPU'    \n",
    "    \n",
    "print(\"Use \" + use_device)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy files to local FS from GCP bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/jupyter/data/bert\n",
    "!mkdir -p /home/jupyter/data/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil -m cp -r -n 'gs://msca-bdp-data-open/bert' '/home/jupyter/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By default Huggingface attempts to download pre-trained models and vocabularies into .cache directory in /root/.cache/\n",
    "- We can alter this behavior by forcing all Huggingface models to go to the explicitly stated cache directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxr-xr-x 3 root root 4096 Jul 26 17:32 huggingface\n",
      "drwxr-xr-x 2 root root 4096 Jun 16 17:35 matplotlib\n",
      "drwxr-xr-x 1 root root 4096 Jul 23 15:30 pip\n"
     ]
    }
   ],
   "source": [
    "!ls -l /root/.cache/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location for Huggingface cache directory\n",
    "cache_dir = '/home/jupyter/data/transformers'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that the first token is always forced to be [CLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_prediction(text):\n",
    "    # Use BERT-base model\n",
    "    model = BertForMaskedLM.from_pretrained('bert-base-uncased', cache_dir = cache_dir)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', cache_dir = cache_dir)\n",
    "\n",
    "    # Encode the text\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "    # Get the index of the [MASK] token\n",
    "    mask_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
    "\n",
    "    # Forward pass through BERT model to get logits\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "    # Get the top 3 token predictions for each [MASK]\n",
    "    all_predictions = []\n",
    "    for index in mask_index:\n",
    "        top_predictions = []\n",
    "        probs = predictions[0, index.item()].softmax(dim=0)\n",
    "        top_3_preds = probs.topk(3)\n",
    "        for i, idx in enumerate(top_3_preds.indices):\n",
    "            token = tokenizer.convert_ids_to_tokens([idx])[0]\n",
    "            top_predictions.append(token)\n",
    "        all_predictions.append(top_predictions)\n",
    "\n",
    "    return all_predictions\n",
    "\n",
    "def replace_mask_with_preds(text, predictions):\n",
    "    for preds in predictions:\n",
    "        for pred in preds:\n",
    "            text_clean = text.replace('[CLS]', '').replace('[SEP]', '')\n",
    "            text_replaced = text_clean.replace('[MASK]', pred, 1)\n",
    "            print(text_replaced.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is time to go home, the class is over.\n",
      "It is time to go out, the class is over.\n",
      "It is time to go back, the class is over.\n"
     ]
    }
   ],
   "source": [
    "maskedText = \"[CLS] It is time to go [MASK], the class is over. [SEP]\"\n",
    "\n",
    "predictions = get_mask_prediction(maskedText)\n",
    "replace_mask_with_preds(maskedText, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TA in our NLP class is very good.\n",
      "TA in our NLP class is very important.\n",
      "TA in our NLP class is very different.\n"
     ]
    }
   ],
   "source": [
    "maskedText = '''[CLS] TA in our NLP class is very [MASK]. [SEP]'''\n",
    "\n",
    "predictions = get_mask_prediction(maskedText)\n",
    "replace_mask_with_preds(maskedText, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP class is so boring, I am falling asleep.\n",
      "NLP class is so boring, I am falling apart.\n",
      "NLP class is so boring, I am falling behind.\n"
     ]
    }
   ],
   "source": [
    "maskedText = '''[CLS] NLP class is so boring, I am falling [MASK]. [SEP]'''\n",
    "\n",
    "predictions = get_mask_prediction(maskedText)\n",
    "replace_mask_with_preds(maskedText, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would like to buy a new iPhone.\n",
      "I would like to buy a pink iPhone.\n",
      "I would like to buy a real iPhone.\n"
     ]
    }
   ],
   "source": [
    "maskedText = '''[CLS] I would like to buy a [MASK] iPhone. [SEP]'''\n",
    "\n",
    "predictions = get_mask_prediction(maskedText)\n",
    "replace_mask_with_preds(maskedText, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is so cold outside, I have to wear a coat, hat and gloves.\n",
      "It is so hot outside, I have to wear a coat, hat and gloves.\n",
      "It is so warm outside, I have to wear a coat, hat and gloves.\n"
     ]
    }
   ],
   "source": [
    "maskedText = '''[CLS] It is so [MASK] outside, I have to wear a coat, hat and gloves. [SEP]'''\n",
    "\n",
    "predictions = get_mask_prediction(maskedText)\n",
    "replace_mask_with_preds(maskedText, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is so hot outside, I am sweating.\n",
      "It is so cold outside, I am sweating.\n",
      "It is so warm outside, I am sweating.\n"
     ]
    }
   ],
   "source": [
    "maskedText = '''[CLS] It is so [MASK] outside, I am sweating. [SEP]'''\n",
    "\n",
    "predictions = get_mask_prediction(maskedText)\n",
    "replace_mask_with_preds(maskedText, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is so late my students are unable to participate it the class\n",
      "It is so late my students are forced to participate it the class\n",
      "It is so late my students are going to participate it the class\n"
     ]
    }
   ],
   "source": [
    "maskedText = '''[CLS] It is so late my students are [MASK] to participate it the class [SEP]'''\n",
    "\n",
    "predictions = get_mask_prediction(maskedText)\n",
    "replace_mask_with_preds(maskedText, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will review the Final Project\n",
      "We shall review the Final Project\n",
      "We must review the Final Project\n"
     ]
    }
   ],
   "source": [
    "maskedText = '''[CLS] We [MASK] review the Final Project [SEP]'''\n",
    "\n",
    "predictions = get_mask_prediction(maskedText)\n",
    "replace_mask_with_preds(maskedText, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wed, 26 July 2023 12:36:44'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "\n",
    "datetime.datetime.now(pytz.timezone('US/Central')).strftime(\"%a, %d %B %Y %H:%M:%S\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
